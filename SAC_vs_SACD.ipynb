{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZZyajqI7xw2D"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install 'git+https://github.com/itaysegev/stable-baselines3.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6aItHAj4yIqt"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import SACD\n",
        "from stable_baselines3 import SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac4NS_RS6I0j",
        "outputId": "a69031be-4c31-46a8-c6ee-03ffa018f47b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/itaysegev/citylearn_utils.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8u_q6fw6yRy",
        "outputId": "488f479d-a635-4b09-e106-9ac905f08329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/citylearn_utils\n"
          ]
        }
      ],
      "source": [
        "%cd citylearn_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Wj24qONQ65ky"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "If3hIUUA73oI"
      },
      "outputs": [],
      "source": [
        "# System operations\n",
        "import os\n",
        "\n",
        "# Date and time\n",
        "from datetime import datetime\n",
        "\n",
        "# type hinting\n",
        "from typing import List, Mapping, Tuple\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from simulation_results import plot_actions, plot_rewards, plot_simulation_summary\n",
        "\n",
        "# Data manipulation\n",
        "import numpy as np\n",
        "\n",
        "# CityLearn\n",
        "from citylearn.data import DataSet\n",
        "from citylearn.citylearn import CityLearnEnv\n",
        "from citylearn.reward_function import RewardFunction, SolarPenaltyReward\n",
        "from citylearn.wrappers import NormalizedObservationWrapper, StableBaselines3Wrapper\n",
        "\n",
        "# Simulation setup\n",
        "from simulation_setup import set_schema_buildings, set_schema_simulation_period, set_active_observations\n",
        "from utils import CustomCallback, get_loader, SACDCallback\n",
        "from custom_rewards import SACCustomReward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KSZDFFki7DAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d697b44-6f47-4377-df5a-fc87023329ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# set all plotted figures without margins\n",
        "plt.rcParams['axes.xmargin'] = 0\n",
        "plt.rcParams['axes.ymargin'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KKqfsfnb8QD8"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = 'citylearn_challenge_2022_phase_all'\n",
        "schema = DataSet.get_schema(DATASET_NAME)\n",
        "\n",
        "root_directory = schema['root_directory']\n",
        "\n",
        "# change the suffix number in the next code line to a\n",
        "# number between 1 and 17 to preview other buildings\n",
        "building_name = 'Building_1'\n",
        "\n",
        "filename = schema['buildings'][building_name]['energy_simulation']\n",
        "filepath = os.path.join(root_directory, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_bxQHHGo72w4"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B-Of148j7_UD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ed3748-1582-41eb-b0fb-54508b26de53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected buildings: ['Building_2', 'Building_7']\n",
            "Selected 7-day period time steps: (7392, 7559)\n",
            "Active observations: ['hour', 'day_type']\n"
          ]
        }
      ],
      "source": [
        "# edit next code line to change number of buildings in simulation\n",
        "BUILDING_COUNT = 2\n",
        "\n",
        " # edit next code line to change number of days in simulation\n",
        "DAY_COUNT = 7\n",
        "\n",
        "# edit next code line to change active observations in simulation\n",
        "# NOTE: More active observations could mean longer trainer time.\n",
        "ACTIVE_OBSERVATIONS = ['hour', 'day_type']\n",
        "\n",
        "schema, buildings = set_schema_buildings(schema, BUILDING_COUNT, RANDOM_SEED)\n",
        "schema, simulation_start_time_step, simulation_end_time_step =\\\n",
        "    set_schema_simulation_period(schema, DAY_COUNT, RANDOM_SEED)\n",
        "schema = set_active_observations(schema, ACTIVE_OBSERVATIONS)\n",
        "\n",
        "print('Selected buildings:', buildings)\n",
        "print(\n",
        "    f'Selected {DAY_COUNT}-day period time steps:',\n",
        "    (simulation_start_time_step, simulation_end_time_step)\n",
        ")\n",
        "print(f'Active observations:', ACTIVE_OBSERVATIONS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3y3xML4j8cLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bac09a3-067b-43b3-bf0b-5a69a963c242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# -------------------- CUSTOMIZE ENVIRONMENT --------------------\n",
        "# Include other observations if needed.\n",
        "# NOTE: More active observations could mean longer trainer time.\n",
        "your_active_observations = [\n",
        "    'hour',\n",
        "    # 'day_type'\n",
        "]\n",
        "\n",
        "# ------------------ SET AGENT HYPERPARAMETERS ------------------\n",
        "your_agent_kwargs = {\n",
        "    'learning_rate': 0.001,\n",
        "    'buffer_size': 1000000,\n",
        "    'learning_starts': 100,\n",
        "    'batch_size': 256,\n",
        "    'tau': 0.005,\n",
        "    'gamma': 0.99,\n",
        "    'train_freq': 1,\n",
        "    'weights_vector': [1, 1],\n",
        "    'policy_kwargs': {'n_reward_components': 2}\n",
        "\n",
        "}\n",
        "\n",
        "# --------------- SET NUMBER OF TRAINING EPISODES ---------------\n",
        "your_episodes = 30\n",
        "\n",
        "# --------------- DEFINE CUSTOM REWARD FUNCTION -----------------\n",
        "class YourCustomReward(SACCustomReward, SolarPenaltyReward):\n",
        "    def __init__(self, env: CityLearnEnv):\n",
        "        r\"\"\"Initialize CustomReward.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        env: Mapping[str, CityLearnEnv]\n",
        "            CityLearn environment instance.\n",
        "        \"\"\"\n",
        "\n",
        "        SolarPenaltyReward.__init__(self, env)\n",
        "        SACCustomReward.__init__(self, env)\n",
        "\n",
        "    def calculate(self) -> List[float]:\n",
        "        r\"\"\"Returns reward for most recent action.\n",
        "\n",
        "        <Provide a description for your custom reward>.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        reward: List[float]\n",
        "            Reward for transition to current timestep.\n",
        "        \"\"\"\n",
        "        reward = [sum(SolarPenaltyReward.calculate(self)), sum(SACCustomReward.calculate(self))]\n",
        "        return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "70DTn8vOCGpx"
      },
      "outputs": [],
      "source": [
        "def train_your_custom_sacd(\n",
        "    agent_kwargs: dict, episodes: int, reward_function: RewardFunction,\n",
        "    building_count: int, day_count: int, active_observations: List[str],\n",
        "    random_seed: int, reference_envs: Mapping[str, CityLearnEnv] = None,\n",
        "    show_figures: bool = None\n",
        ") -> dict:\n",
        "    \"\"\"Trains a custom soft-actor critic (SACD) agent on a custom environment.\n",
        "\n",
        "    Trains an SAC agent using a custom environment and agent hyperparamter\n",
        "    setup and plots the key performance indicators (KPIs), actions and\n",
        "    rewards from training and evaluating the agent.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    agent_kwargs: dict\n",
        "        Defines the hyperparameters used to initialize the SAC agent.\n",
        "    episodes: int\n",
        "        Number of episodes to train the agent for.\n",
        "    reward_function: RewardFunction\n",
        "        A base or custom reward function class.\n",
        "    building_count: int\n",
        "        Number of buildings to set as active in schema.\n",
        "    day_count: int\n",
        "        Number of simulation days.\n",
        "    active_observations: List[str]\n",
        "        Names of observations to set active to be passed to control agent.\n",
        "    random_seed: int\n",
        "        Seed for pseudo-random number generator.\n",
        "    reference_envs: Mapping[str, CityLearnEnv], default: None\n",
        "        Mapping of user-defined control agent names to environments\n",
        "        the agents have been used to control.\n",
        "    show_figures: bool, default: False\n",
        "        Indicate if summary figures should be plotted at the end of\n",
        "        evaluation.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    result: dict\n",
        "        Results from training the agent as well as some input variables\n",
        "        for reference including the following value keys:\n",
        "\n",
        "            * random_seed: int\n",
        "            * env: CityLearnEnv\n",
        "            * model: SAC\n",
        "            * actions: List[float]\n",
        "            * rewards: List[float]\n",
        "            * agent_kwargs: dict\n",
        "            * episodes: int\n",
        "            * reward_function: RewardFunction\n",
        "            * buildings: List[str]\n",
        "            * simulation_start_time_step: int\n",
        "            * simulation_end_time_step: int\n",
        "            * active_observations: List[str]\n",
        "            * train_start_timestamp: datetime\n",
        "            * train_end_timestamp: datetime\n",
        "    \"\"\"\n",
        "\n",
        "    # get schema\n",
        "    schema = DataSet.get_schema('citylearn_challenge_2022_phase_all')\n",
        "\n",
        "    # select buildings\n",
        "    schema, buildings = set_schema_buildings(\n",
        "        schema, building_count, random_seed\n",
        "    )\n",
        "    print('Selected buildings:', buildings)\n",
        "\n",
        "    # select days\n",
        "    schema, simulation_start_time_step, simulation_end_time_step =\\\n",
        "        set_schema_simulation_period(schema, day_count, random_seed)\n",
        "    print(\n",
        "        f'Selected {day_count}-day period time steps:',\n",
        "        (simulation_start_time_step, simulation_end_time_step)\n",
        "    )\n",
        "\n",
        "    # set active observations\n",
        "    schema = set_active_observations(schema, active_observations)\n",
        "    print(f'Active observations:', active_observations)\n",
        "\n",
        "    # initialize environment\n",
        "    env = CityLearnEnv(schema, central_agent=True)\n",
        "    sacr_env = CityLearnEnv(schema, central_agent=True)\n",
        "\n",
        "    # set reward function\n",
        "    env.reward_function = reward_function(env=env)\n",
        "    sacr_env.reward_function = SACCustomReward(sacr_env)\n",
        "\n",
        "    # wrap environment\n",
        "    env = NormalizedObservationWrapper(env)\n",
        "    env = StableBaselines3Wrapper(env)\n",
        "\n",
        "    sacr_env = NormalizedObservationWrapper(sacr_env)\n",
        "    sacr_env = StableBaselines3Wrapper(sacr_env)\n",
        "\n",
        "    # initialize agent\n",
        "\n",
        "    model = SACD('MultiPerspectivePolicy', env, **agent_kwargs, seed=random_seed)\n",
        "    sacr_model = SAC(policy='MlpPolicy', env=sacr_env, seed=RANDOM_SEED)\n",
        "\n",
        "    # initialize loader\n",
        "    total_timesteps = episodes*(env.time_steps - 1)\n",
        "    print('Number of episodes to train:', episodes)\n",
        "\n",
        "    # initialize SAC loader\n",
        "    sac_modr_loader = get_loader(max=total_timesteps)\n",
        "    print('Train SAC agent...')\n",
        "    display(sac_modr_loader)\n",
        "\n",
        "    # train SAC agent\n",
        "    sacr_callback = CustomCallback(env=sacr_env, loader=sac_modr_loader)\n",
        "    sacr_model = sacr_model.learn(total_timesteps=total_timesteps,\n",
        "                                  callback=sacr_callback)\n",
        "\n",
        "    # Evaluate the trained SAC model\n",
        "    observations = sacr_env.reset()\n",
        "    sacr_actions_list = []\n",
        "\n",
        "    while not sacr_env.done:\n",
        "        actions, _ = sacr_model.predict(observations, deterministic=True)\n",
        "        observations, _, _, _ = sacr_env.step(actions)\n",
        "        sacr_actions_list.append(actions)\n",
        "\n",
        "    fig = plot_actions(sacr_actions_list, 'SAC Actions', sacr_env)\n",
        "    plt.show()\n",
        "    reference_envs={'SAC': sacr_env}\n",
        "\n",
        "    # initialize SACD loader\n",
        "    loader = get_loader(max=total_timesteps)\n",
        "    print('Train SACD agent...')\n",
        "    display(loader)\n",
        "\n",
        "    # initialize callback\n",
        "    weights_vector = agent_kwargs['weights_vector']\n",
        "    callback = SACDCallback(env=env, loader=loader, weights_vector=weights_vector)\n",
        "\n",
        "    # train SACD agent\n",
        "    train_start_timestamp = datetime.utcnow()\n",
        "    model = model.learn(total_timesteps=total_timesteps, callback=callback)\n",
        "    train_end_timestamp = datetime.utcnow()\n",
        "\n",
        "    # evaluate SACD agent\n",
        "    observations = env.reset()\n",
        "    actions_list = []\n",
        "\n",
        "    while not env.done:\n",
        "        actions, _ = model.predict(observations, deterministic=True)\n",
        "        observations, _, _, _ = env.step(actions)\n",
        "        actions_list.append(actions)\n",
        "\n",
        "    # get rewards\n",
        "    rewards = callback.reward_history[:episodes]\n",
        "\n",
        "    # plot summary and compare with other control results\n",
        "    if show_figures is not None and show_figures:\n",
        "        env_id = 'SACD'\n",
        "\n",
        "        reference_envs = {env_id: env, **reference_envs}\n",
        "        plot_simulation_summary(reference_envs)\n",
        "\n",
        "        # plot actions\n",
        "        plot_actions(actions_list, f'{env_id} Actions', env)\n",
        "\n",
        "        # plot rewards\n",
        "        _, ax = plt.subplots(1, 1, figsize=(5, 2))\n",
        "        ax = plot_rewards(ax, rewards, f'{env_id} Rewards')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    return {\n",
        "        'random_seed': random_seed,\n",
        "        'env': env,\n",
        "        'model': model,\n",
        "        'actions': actions_list,\n",
        "        'rewards': rewards,\n",
        "        'agent_kwargs': agent_kwargs,\n",
        "        'episodes': episodes,\n",
        "        'reward_function': reward_function,\n",
        "        'buildings': buildings,\n",
        "        'simulation_start_time_step': simulation_start_time_step,\n",
        "        'simulation_end_time_step': simulation_end_time_step,\n",
        "        'active_observations': active_observations,\n",
        "        'train_start_timestamp': train_start_timestamp,\n",
        "        'train_end_timestamp': train_end_timestamp,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEoZer9F-XiV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "4de1f3b5f1fb4145b2b2b42ce48c920d",
            "fb1f76df446f49b9ae390b9e4f8552fa",
            "dcd6c3316af54878a2f457a22d41e37a"
          ]
        },
        "outputId": "11b2fa48-64b3-45ea-9674-7cd4ed0ecbd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected buildings: ['Building_2', 'Building_7']\n",
            "Selected 7-day period time steps: (7392, 7559)\n",
            "Active observations: ['hour']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of episodes to train: 30\n",
            "Train SAC agent...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "IntProgress(value=0, description='Simulating:', max=5010, style=ProgressStyle(bar_color='maroon'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4de1f3b5f1fb4145b2b2b42ce48c920d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "your_results = train_your_custom_sacd(\n",
        "    agent_kwargs=your_agent_kwargs,\n",
        "    episodes=your_episodes,\n",
        "    reward_function=YourCustomReward,\n",
        "    building_count=BUILDING_COUNT,\n",
        "    day_count=DAY_COUNT,\n",
        "    active_observations=your_active_observations,\n",
        "    random_seed=RANDOM_SEED,\n",
        "    show_figures=True,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4de1f3b5f1fb4145b2b2b42ce48c920d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "Simulating:",
            "description_allow_html": false,
            "layout": "IPY_MODEL_fb1f76df446f49b9ae390b9e4f8552fa",
            "max": 5010,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcd6c3316af54878a2f457a22d41e37a",
            "tabbable": null,
            "tooltip": null,
            "value": 3993
          }
        },
        "fb1f76df446f49b9ae390b9e4f8552fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcd6c3316af54878a2f457a22d41e37a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": "maroon",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}